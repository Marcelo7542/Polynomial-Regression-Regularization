Polynomial Regression

Criei este projeto para implementar uma regress√£o polinomial de formal manual usando Python e bibliotecas como NumPy, Matplotlib e Scikit-learn. 
Estava curioso para observar o desempenho do modelo manual.

Passos Tomados

Gera√ß√£o dos Dados: 

Primeiramente, gerei dados sint√©ticos usando uma f√≥rmula polinomial de grau 5 para criar os valores de  Y em fun√ß√£o de 
X, com adi√ß√£o de ru√≠do aleat√≥rio. 
O conjunto de dados foi composto por 10.000 pontos, e os valores de X foram gerados aleatoriamente na faixa de 2 a 5.

Visualiza√ß√£o Inicial: 

Visualizei os dados gerados em gr√°fico de dispers√£o para observar a distribui√ß√£o dos pontos e a rela√ß√£o entre 
X e Y.

Transforma√ß√£o Polinomial e Normaliza√ß√£o: 

Para aplicar a regress√£o polinomial, transformei os dados de entrada 
X em suas respectivas caracter√≠sticas polinomiais de grau 5 utilizando PolynomialFeatures do Scikit-learn. 
Tamb√©m normalizei os dados usando StandardScaler para padronizar a escala das vari√°veis.

Treinamento do Modelo de Regress√£o Linear: 

A regress√£o linear foi aplicada ao conjunto de dados transformado e normalizado. 
O modelo foi treinado e avaliei os coeficientes do modelo, o intercepto, e o desempenho do ajuste utilizando o coeficiente de determina√ß√£o 
ùëÖ¬≤.

Curvas de Aprendizado: 

Para analisar o comportamento do modelo em diferentes tamanhos de conjunto de treinamento, 
gerei curvas de aprendizado usando learning_curve() que mostraram os erros de treinamento e valida√ß√£o em fun√ß√£o do tamanho do conjunto de dados. 
Utilizei o erro quadr√°tico m√©dio (RMSE) como m√©trica de avalia√ß√£o.

Pipeline de Regress√£o Polinomial: 

Para otimizar o processo de transforma√ß√£o e treinamento, utilizei um pipeline que combina as transforma√ß√µes polinomiais, 
a normaliza√ß√£o e a regress√£o linear em uma √∫nica etapa. As curvas de aprendizado foram novamente geradas para avaliar o desempenho do pipeline.

Implementa√ß√£o Manual da Transforma√ß√£o Polinomial: 

Para fins educativos, implementei uma vers√£o manual das transforma√ß√µes polinomiais e da normaliza√ß√£o dos dados, 
sem utilizar as fun√ß√µes prontas do Scikit-learn. Utilizando √°lgebra linear, 
calculei a solu√ß√£o dos coeficientes do modelo utilizando o m√©todo dos m√≠nimos quadrados.

Avalia√ß√£o do Modelo: 

Por fim, apliquei o modelo treinado a um novo conjunto de dados gerado de forma semelhante ao primeiro, 
para calcular o erro quadr√°tico m√©dio (MSE) entre as previs√µes e os valores reais, avaliando o desempenho do modelo em dados n√£o vistos.

Tecnologias Utilizadas

Python

NumPy

Matplotlib

Scikit-learn



Regularization Ridge

Criei este projeto para explorar a regulariza√ß√£o em modelos de regress√£o, especificamente utilizando Ridge Regression. 
Utilizei a abordagem polinomial para gerar dados, foram aplicadas t√©cnicas de regulariza√ß√£o, como o Ridge Regression e o m√©todo de stochastic gradient de forma manual.

Passos Tomados

Gera√ß√£o dos Dados: 

Similar ao projeto anterior, foram gerados dados sint√©ticos utilizando uma fun√ß√£o polinomial de grau 5, com adi√ß√£o de ru√≠do aleat√≥rio. 

O conjunto de dados foi composto por 10.000 pontos.

Visualiza√ß√£o Inicial: Os dados gerados foram visualizados em um gr√°fico de dispers√£o para observar a distribui√ß√£o dos pontos e a rela√ß√£o entre as vari√°veis 
X e Y.

Transforma√ß√£o Polinomial e Normaliza√ß√£o: 

A transforma√ß√£o polinomial de grau 6 foi aplicada aos dados, expandindo as caracter√≠sticas de X para incluir pot√™ncias maiores. 
Em seguida, os dados foram normalizados utilizando a m√©dia e o desvio padr√£o das caracter√≠sticas, para garantir que todas as vari√°veis estivessem na mesma escala.

Regulariza√ß√£o Ridge Manual: 

Para entender melhor o impacto da regulariza√ß√£o, implementei a regulariza√ß√£o Ridge manualmente, adicionando o termo de regulariza√ß√£o 
Œª √† f√≥rmula dos m√≠nimos quadrados, que ajuda a prevenir overfitting.

Gradiente Descendente com Regulariza√ß√£o: 

A seguir, implementei o gradiente descendente com regulariza√ß√£o L2 (Ridge) utilizando uma taxa de aprendizado (eta) fixa e o n√∫mero de itera√ß√µes ajustado. 
O modelo foi treinado por 10.000 √©pocas para minimizar o erro quadr√°tico m√©dio.

Gradiente Descendente Estoc√°stico (Mini-Batch): 

Para melhorar a efici√™ncia, apliquei o Stochastic gradient descent (SGD) com mini-batches. 
Implementei este m√©todo para atualizar os par√¢metros em pequenos lotes de dados, o que pode acelerar o processo de treinamento.

Uso de Ridge Regression: 

O modelo de Ridge Regression foi implementado utilizando o Ridge do Scikit-learn. O par√¢metro de regulariza√ß√£o 
ùõº foi ajustado para encontrar o melhor valor, utilizando o m√©todo de valida√ß√£o cruzada RidgeCV.

Avalia√ß√£o do Modelo: 

O modelo foi avaliado com dados de teste, calculando o erro quadr√°tico m√©dio (MSE) entre as previs√µes do modelo e os valores reais. 
O desempenho do modelo foi comparado entre os m√©todos manuais e os m√©todos do Scikit-learn.

RidgeCV: 

Finalmente, apliquei o RidgeCV, que realiza a busca pelo melhor valor de 
Œ± utilizando valida√ß√£o cruzada. O modelo foi treinado, e o melhor valor de 
Œ± foi encontrado.

Tecnologias Utilizadas

Python

NumPy

Matplotlib

Scikit-learn




Lasso & ElasticNet Regularization


Criei este projeto para explorar a regulariza√ß√£o Lasso e ElasticNet em modelos de regress√£o polinomial. 

Utilizei uma abordagem similar √† dos projetos anteriores, 
abordei a regulariza√ß√£o tanto no formato L1 (Lasso) quanto na combina√ß√£o L1-L2 (ElasticNet).

Passos Tomados

Gera√ß√£o dos Dados: 

Assim como nos exemplos anteriores, gerei um conjunto de dados sint√©ticos com 10.000 pontos, onde a vari√°vel 
Y √© definida por um polin√¥mio de grau 5 com ru√≠do adicionado.

Visualiza√ß√£o Inicial: 

Os dados gerados foram visualizados em um gr√°fico de dispers√£o para examinar a rela√ß√£o entre 
X e Y.

Transforma√ß√£o Polinomial e Normaliza√ß√£o: 

Os dados foram transformados para um espa√ßo polinomial de grau 6 e, em seguida, normalizados utilizando a m√©dia e o desvio padr√£o.

Regulariza√ß√£o Lasso e ElasticNet Manual: 

Implementei a regulariza√ß√£o Lasso (L1) e ElasticNet (L1-L2) manualmente no gradiente descendente, utilizei o termo de regulariza√ß√£o apropriado para cada caso. 

Early Stopping: 

Para fins educativos, implementei a t√©cnica de early stopping de forma manual, 
onde o treinamento √© interrompido se n√£o houver melhorias significativas no MSE ap√≥s um certo n√∫mero de √©pocas.

Uso de Lasso e ElasticNet do Scikit-learn: 

Para compara√ß√£o, utilizei as implementa√ß√µes de Lasso e ElasticNet do sklearn e avaliei seu desempenho em termos de erro quadr√°tico m√©dio.


Tecnologias Utilizadas

Python

NumPy

Matplotlib

Scikit-learn
