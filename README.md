English description below

Polynomial Regression

Criei este projeto para implementar uma regress√£o polinomial de formal manual usando Python e bibliotecas como NumPy, Matplotlib e Scikit-learn. 
Estava curioso para observar o desempenho do modelo manual.

Passos Tomados

Gera√ß√£o dos Dados: 

Primeiramente, gerei dados sint√©ticos usando uma f√≥rmula polinomial de grau 5 para criar os valores de  Y em fun√ß√£o de 
X, com adi√ß√£o de ru√≠do aleat√≥rio. 
O conjunto de dados foi composto por 10.000 pontos, e os valores de X foram gerados aleatoriamente na faixa de 2 a 5.

Visualiza√ß√£o Inicial: 

Visualizei os dados gerados em gr√°fico de dispers√£o para observar a distribui√ß√£o dos pontos e a rela√ß√£o entre 
X e Y.

Transforma√ß√£o Polinomial e Normaliza√ß√£o: 

Para aplicar a regress√£o polinomial, transformei os dados de entrada 
X em suas respectivas caracter√≠sticas polinomiais de grau 5 utilizando PolynomialFeatures do Scikit-learn. 
Tamb√©m normalizei os dados usando StandardScaler para padronizar a escala das vari√°veis.

Treinamento do Modelo de Regress√£o Linear: 

A regress√£o linear foi aplicada ao conjunto de dados transformado e normalizado. 
O modelo foi treinado e avaliei os coeficientes do modelo, o intercepto, e o desempenho do ajuste utilizando o coeficiente de determina√ß√£o 
ùëÖ¬≤.

Curvas de Aprendizado: 

Para analisar o comportamento do modelo em diferentes tamanhos de conjunto de treinamento, 
gerei curvas de aprendizado usando learning_curve() que mostraram os erros de treinamento e valida√ß√£o em fun√ß√£o do tamanho do conjunto de dados. 
Utilizei o erro quadr√°tico m√©dio (RMSE) como m√©trica de avalia√ß√£o.

Pipeline de Regress√£o Polinomial: 

Para otimizar o processo de transforma√ß√£o e treinamento, utilizei um pipeline que combina as transforma√ß√µes polinomiais, 
a normaliza√ß√£o e a regress√£o linear em uma √∫nica etapa. As curvas de aprendizado foram novamente geradas para avaliar o desempenho do pipeline.

Implementa√ß√£o Manual da Transforma√ß√£o Polinomial: 

Para fins educativos, implementei uma vers√£o manual das transforma√ß√µes polinomiais e da normaliza√ß√£o dos dados, 
sem utilizar as fun√ß√µes prontas do Scikit-learn. Utilizando √°lgebra linear, 
calculei a solu√ß√£o dos coeficientes do modelo utilizando o m√©todo dos m√≠nimos quadrados.

Avalia√ß√£o do Modelo: 

Por fim, apliquei o modelo treinado a um novo conjunto de dados gerado de forma semelhante ao primeiro, 
para calcular o erro quadr√°tico m√©dio (MSE) entre as previs√µes e os valores reais, avaliando o desempenho do modelo em dados n√£o vistos.

Tecnologias Utilizadas

Python

NumPy

Matplotlib

Scikit-learn



Regularization Ridge

Criei este projeto para explorar a regulariza√ß√£o em modelos de regress√£o, especificamente utilizando Ridge Regression. 
Utilizei a abordagem polinomial para gerar dados, foram aplicadas t√©cnicas de regulariza√ß√£o, como o Ridge Regression e o m√©todo de stochastic gradient de forma manual.

Passos Tomados

Gera√ß√£o dos Dados: 

Similar ao projeto anterior, foram gerados dados sint√©ticos utilizando uma fun√ß√£o polinomial de grau 5, com adi√ß√£o de ru√≠do aleat√≥rio. 

O conjunto de dados foi composto por 10.000 pontos.

Visualiza√ß√£o Inicial: Os dados gerados foram visualizados em um gr√°fico de dispers√£o para observar a distribui√ß√£o dos pontos e a rela√ß√£o entre as vari√°veis 
X e Y.

Transforma√ß√£o Polinomial e Normaliza√ß√£o: 

A transforma√ß√£o polinomial de grau 6 foi aplicada aos dados, expandindo as caracter√≠sticas de X para incluir pot√™ncias maiores. 
Em seguida, os dados foram normalizados utilizando a m√©dia e o desvio padr√£o das caracter√≠sticas, para garantir que todas as vari√°veis estivessem na mesma escala.

Regulariza√ß√£o Ridge Manual: 

Para entender melhor o impacto da regulariza√ß√£o, implementei a regulariza√ß√£o Ridge manualmente, adicionando o termo de regulariza√ß√£o 
Œª √† f√≥rmula dos m√≠nimos quadrados, que ajuda a prevenir overfitting.

Gradiente Descendente com Regulariza√ß√£o: 

A seguir, implementei o gradiente descendente com regulariza√ß√£o L2 (Ridge) utilizando uma taxa de aprendizado (eta) fixa e o n√∫mero de itera√ß√µes ajustado. 
O modelo foi treinado por 10.000 √©pocas para minimizar o erro quadr√°tico m√©dio.

Gradiente Descendente Estoc√°stico (Mini-Batch): 

Para melhorar a efici√™ncia, apliquei o Stochastic gradient descent (SGD) com mini-batches. 
Implementei este m√©todo para atualizar os par√¢metros em pequenos lotes de dados, o que pode acelerar o processo de treinamento.

Uso de Ridge Regression: 

O modelo de Ridge Regression foi implementado utilizando o Ridge do Scikit-learn. O par√¢metro de regulariza√ß√£o 
ùõº foi ajustado para encontrar o melhor valor, utilizando o m√©todo de valida√ß√£o cruzada RidgeCV.

Avalia√ß√£o do Modelo: 

O modelo foi avaliado com dados de teste, calculando o erro quadr√°tico m√©dio (MSE) entre as previs√µes do modelo e os valores reais. 
O desempenho do modelo foi comparado entre os m√©todos manuais e os m√©todos do Scikit-learn.

RidgeCV: 

Finalmente, apliquei o RidgeCV, que realiza a busca pelo melhor valor de 
Œ± utilizando valida√ß√£o cruzada. O modelo foi treinado, e o melhor valor de 
Œ± foi encontrado.

Tecnologias Utilizadas

Python

NumPy

Matplotlib

Scikit-learn




Lasso & ElasticNet Regularization


Criei este projeto para explorar a regulariza√ß√£o Lasso e ElasticNet em modelos de regress√£o polinomial. 

Utilizei uma abordagem similar √† dos projetos anteriores, 
abordei a regulariza√ß√£o tanto no formato L1 (Lasso) quanto na combina√ß√£o L1-L2 (ElasticNet).

Passos Tomados

Gera√ß√£o dos Dados: 

Assim como nos exemplos anteriores, gerei um conjunto de dados sint√©ticos com 10.000 pontos, onde a vari√°vel 
Y √© definida por um polin√¥mio de grau 5 com ru√≠do adicionado.

Visualiza√ß√£o Inicial: 

Os dados gerados foram visualizados em um gr√°fico de dispers√£o para examinar a rela√ß√£o entre 
X e Y.

Transforma√ß√£o Polinomial e Normaliza√ß√£o: 

Os dados foram transformados para um espa√ßo polinomial de grau 6 e, em seguida, normalizados utilizando a m√©dia e o desvio padr√£o.

Regulariza√ß√£o Lasso e ElasticNet Manual: 

Implementei a regulariza√ß√£o Lasso (L1) e ElasticNet (L1-L2) manualmente no gradiente descendente, utilizei o termo de regulariza√ß√£o apropriado para cada caso. 

Early Stopping: 

Para fins educativos, implementei a t√©cnica de early stopping de forma manual, 
onde o treinamento √© interrompido se n√£o houver melhorias significativas no MSE ap√≥s um certo n√∫mero de √©pocas.

Uso de Lasso e ElasticNet do Scikit-learn: 

Para compara√ß√£o, utilizei as implementa√ß√µes de Lasso e ElasticNet do sklearn e avaliei seu desempenho em termos de erro quadr√°tico m√©dio.


Tecnologias Utilizadas

Python

NumPy

Matplotlib

Scikit-learn





Polynomial Regression

I created this project to implement polynomial regression manually using Python and libraries such as NumPy, Matplotlib, and Scikit-learn. 
I was curious to observe the model's performance through a manual implementation.

Steps Taken

Data Generation: 

First, I generated synthetic data using a degree 5 polynomial formula to create the values of Y as a function of X, with the addition of random noise. The dataset consisted of 10,000 points, and the values of X were randomly generated in the range of 2 to 5.

Initial Visualization: 

I visualized the generated data in a scatter plot to observe the distribution of points and the relationship between X and Y.

Polynomial Transformation and Normalization: 

To apply polynomial regression, I transformed the input data X into its respective degree 5 polynomial features using PolynomialFeatures from Scikit-learn. I also normalized the data using StandardScaler to standardize the variable scales.

Training the Linear Regression Model: 

Linear regression was applied to the transformed and normalized dataset. The model was trained, and I evaluated the model's coefficients, intercept, and the performance of the fit using the coefficient of determination ùëÖ¬≤.

Learning Curves: 

To analyze the model's behavior with different training set sizes, I generated learning curves using learning_curve() to show training and validation errors as a function of dataset size. I used the root mean squared error (RMSE) as the evaluation metric.

Polynomial Regression Pipeline: 

To optimize the transformation and training process, I used a pipeline combining polynomial transformations, normalization, and linear regression into a single step. Learning curves were generated again to evaluate the pipeline's performance.

Manual Polynomial Transformation Implementation: 

For educational purposes, I implemented a manual version of the polynomial transformations and data normalization without using Scikit-learn's built-in functions. Using linear algebra, I computed the solution for the model's coefficients using the least squares method.

Model Evaluation:

Finally, I applied the trained model to a new dataset generated similarly to the first, to calculate the mean squared error (MSE) between the predictions and actual values, evaluating the model's performance on unseen data.

Technologies Used:

Python

NumPy

Matplotlib

Scikit-learn

Ridge Regularization

I created this project to explore regularization in regression models, specifically using Ridge Regression. I used a polynomial approach to generate data and applied regularization techniques, such as Ridge Regression and manual stochastic gradient methods.

Steps Taken

Data Generation: 

Similar to the previous project, synthetic data was generated using a degree 5 polynomial function with random noise added. The dataset consisted of 10,000 points.

Initial Visualization: 

The generated data was visualized in a scatter plot to observe the distribution of points and the relationship between X and Y.

Polynomial Transformation and Normalization: 

A degree 6 polynomial transformation was applied to the data, expanding the features of X to include higher powers. Then, the data was normalized using the mean and standard deviation of the features to ensure all variables were on the same scale.

Manual Ridge Regularization: 

To better understand the impact of regularization, I manually implemented Ridge regularization by adding the regularization term 
Œª to the least squares formula, which helps prevent overfitting.

Gradient Descent with Regularization: 

Next, I implemented gradient descent with L2 regularization (Ridge) using a fixed learning rate (Œ∑) and an adjusted number of iterations. The model was trained for 10,000 epochs to minimize the mean squared error.

Stochastic Gradient Descent (Mini-Batch): 

To improve efficiency, I applied Stochastic Gradient Descent (SGD) with mini-batches. This method updates the parameters in small batches of data, which can speed up the training process.

Ridge Regression Use: 

The Ridge Regression model was implemented using the Ridge class from Scikit-learn. The regularization parameter Œ± was adjusted to find the best value using the RidgeCV cross-validation method.

Model Evaluation: 

The model was evaluated with test data by calculating the mean squared error (MSE) between the model's predictions and the actual values. The model's performance was compared between the manual methods and Scikit-learn's methods.

RidgeCV:

Finally, I applied RidgeCV, which performs a search for the best Œ± value using cross-validation. The model was trained, and the optimal 
Œ± value was found.

Technologies Used:

Python

NumPy

Matplotlib

Scikit-learn

Lasso & ElasticNet Regularization

I created this project to explore Lasso and ElasticNet regularization in polynomial regression models. I used a similar approach to the previous projects, addressing regularization in both L1 (Lasso) and the combination of L1-L2 (ElasticNet).

Steps Taken

Data Generation: 

As in the previous examples, I generated a synthetic dataset with 10,000 points, where the variable Y is defined by a degree 5 polynomial with added noise.

Initial Visualization: 

The generated data was visualized in a scatter plot to examine the relationship between X and Y.

Polynomial Transformation and Normalization: 

The data was transformed into a degree 6 polynomial space and then normalized using the mean and standard deviation.

Manual Lasso and ElasticNet Regularization: 

I manually implemented Lasso (L1) and ElasticNet (L1-L2) regularization in gradient descent, using the appropriate regularization term for each case.

Early Stopping: 

For educational purposes, I implemented early stopping manually, where training is interrupted if no significant improvement in MSE is observed after a set number of epochs.

Using Lasso and ElasticNet from Scikit-learn: 

For comparison, I used the Scikit-learn implementations of Lasso and ElasticNet and evaluated their performance in terms of mean squared error.

Technologies Used:

Python

NumPy

Matplotlib

Scikit-learn
